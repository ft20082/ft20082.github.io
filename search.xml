<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hadoop 文件写入失败分析</title>
    <url>/posts/2119768524/</url>
    <content><![CDATA[<h1 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h1><p>集群的 spark 作业报错，内容为</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="number">22</span>/<span class="number">06</span>/<span class="number">01</span> <span class="number">14</span>:<span class="number">03</span>:<span class="number">43</span> ERROR hdfs.DFSClient: Failed to close file: /user/spark/applicationHistory/application_1640831905154_1811500_1.inprogress with inode: <span class="number">3477505774</span></span><br><span class="line">java.io.IOException: Unable to close file because the last blockBP-<span class="number">293537485</span>-<span class="number">172.18</span><span class="number">.0</span><span class="number">.36</span>-<span class="number">1564638411317</span>:blk_3640214045_2566927779 does not have enough number of replicas.</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:<span class="number">966</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:<span class="number">909</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:<span class="number">892</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:<span class="number">847</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten(DFSClient.java:<span class="number">609</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSClient.closeOutputStreams(DFSClient.java:<span class="number">641</span>)</span><br><span class="line">	at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:<span class="number">1278</span>)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:<span class="number">3353</span>)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:<span class="number">3370</span>)</span><br><span class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:<span class="number">511</span>)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:<span class="number">266</span>)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:<span class="number">1149</span>)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:<span class="number">624</span>)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:<span class="number">748</span>)</span><br></pre></td></tr></table></figure>

<h1 id="排查流程"><a href="#排查流程" class="headerlink" title="排查流程"></a>排查流程</h1><h2 id="Hadoop-客户端排查"><a href="#Hadoop-客户端排查" class="headerlink" title="Hadoop 客户端排查"></a>Hadoop 客户端排查</h2><p>报错主要原因是  <code>HADOOP</code>  客户端写入文件没有完成，重试多次之后会抛出此错误。</p>
<p>具体代码在  <code>org.apache.hadoop.hdfs.DFSOutputStream</code> 的 <code>completeFile</code> 方法中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">completeFile</span><span class="params">(ExtendedBlock last)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="type">long</span> <span class="variable">localstart</span> <span class="operator">=</span> Time.monotonicNow();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">DfsClientConf</span> <span class="variable">conf</span> <span class="operator">=</span> dfsClient.getConf();</span><br><span class="line">    <span class="comment">// 初始休眠时间</span></span><br><span class="line">    <span class="type">long</span> <span class="variable">sleeptime</span> <span class="operator">=</span> conf.getBlockWriteLocateFollowingInitialDelayMs();</span><br><span class="line">    <span class="comment">// 最大休眠时间</span></span><br><span class="line">    <span class="type">long</span> <span class="variable">maxSleepTime</span> <span class="operator">=</span> conf.getBlockWriteLocateFollowingMaxDelayMs();</span><br><span class="line">    <span class="type">boolean</span> <span class="variable">fileComplete</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="comment">// 获取重试次数</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">retries</span> <span class="operator">=</span> conf.getNumBlockWriteLocateFollowingRetry();</span><br><span class="line">    <span class="keyword">while</span> (!fileComplete) &#123;</span><br><span class="line">      <span class="comment">// 通过 rpc 调用 namenode 的 complete 方法，来获取返回结果</span></span><br><span class="line">      fileComplete =</span><br><span class="line">          dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);</span><br><span class="line">      <span class="comment">// 如果没有完成，则重试，如果重试 5 次还是失败，则抛出异常</span></span><br><span class="line">      <span class="keyword">if</span> (!fileComplete) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">int</span> <span class="variable">hdfsTimeout</span> <span class="operator">=</span> conf.getHdfsTimeout();</span><br><span class="line">        <span class="keyword">if</span> (!dfsClient.clientRunning</span><br><span class="line">            || (hdfsTimeout &gt; <span class="number">0</span></span><br><span class="line">                &amp;&amp; localstart + hdfsTimeout &lt; Time.monotonicNow())) &#123;</span><br><span class="line">          <span class="type">String</span> <span class="variable">msg</span> <span class="operator">=</span> <span class="string">&quot;Unable to close file because dfsclient &quot;</span> +</span><br><span class="line">              <span class="string">&quot; was unable to contact the HDFS servers. clientRunning &quot;</span> +</span><br><span class="line">              dfsClient.clientRunning + <span class="string">&quot; hdfsTimeout &quot;</span> + hdfsTimeout;</span><br><span class="line">          DFSClient.LOG.info(msg);</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IOException</span>(msg);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> (<span class="type">TraceScope</span> <span class="variable">scope</span> <span class="operator">=</span> dfsClient.getTracer()</span><br><span class="line">            .newScope(<span class="string">&quot;DFSOutputStream#completeFile: Retry&quot;</span>)) &#123;</span><br><span class="line">          scope.addKVAnnotation(<span class="string">&quot;retries left&quot;</span>, retries);</span><br><span class="line">          scope.addKVAnnotation(<span class="string">&quot;sleeptime (sleeping for)&quot;</span>, sleeptime);</span><br><span class="line">          <span class="keyword">if</span> (retries == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IOException</span>(<span class="string">&quot;Unable to close file because the last block &quot;</span></span><br><span class="line">                + last + <span class="string">&quot; does not have enough number of replicas.&quot;</span>);</span><br><span class="line">          &#125;</span><br><span class="line">          retries--;</span><br><span class="line">          Thread.sleep(sleeptime);</span><br><span class="line">          sleeptime = calculateDelayForNextRetry(sleeptime, maxSleepTime);</span><br><span class="line">          <span class="keyword">if</span> (Time.monotonicNow() - localstart &gt; <span class="number">5000</span>) &#123;</span><br><span class="line">            DFSClient.LOG.info(<span class="string">&quot;Could not complete &quot;</span> + src + <span class="string">&quot; retrying...&quot;</span>);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException ie) &#123;</span><br><span class="line">          DFSClient.LOG.warn(<span class="string">&quot;Caught exception &quot;</span>, ie);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>其中重试次数为  <code>int retries = conf.getNumBlockWriteLocateFollowingRetry();</code></p>
<p>获取的配置项是 <code>dfs.client.block.write.locateFollowingBlock.retries</code>，解释在 <a href="https://hadoop.apache.org/docs/r3.2.3/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">hdfs-default.xml</a> 文件里，默认值是 5 。</p>
<h2 id="集群监控"><a href="#集群监控" class="headerlink" title="集群监控"></a>集群监控</h2><p>从集群监控发现，当时主要异常的指标有 </p>
<ol>
<li>RPC 处理时间超时</li>
<li>RPC 队列时间超过</li>
<li>定时等待的线程变少了</li>
<li>等待的线程变多了</li>
</ol>
<p>从以上 4 个主要异常点来看，表象问题是rpc 处理时间和队列时间偏长</p>
<h1 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h1><p>客户端报错的原因是  <code>NameNode</code>  没有及时确认文件写入完成，重试逻辑是每次休眠是上一次休眠的 2 倍时间，直到最大等待时间。等待初始时间为  <code>dfs.client.block.write.locateFollowingBlock.initial.delay.ms</code> 默认值是 400ms，等待最大时间 <code>dfs.client.locateFollowingBlock.max.delay.ms</code> 的默认值是 60000，即 60 秒。重试 5 次需要等待 <code>400</code> &#x2F; <code>800</code> &#x2F; <code>1600</code> &#x2F; <code>3200</code> &#x2F; <code>6400</code>  共需要 12400 毫秒。</p>
<h1 id="NameNode-RPC-延迟分析"><a href="#NameNode-RPC-延迟分析" class="headerlink" title="NameNode RPC 延迟分析"></a>NameNode RPC 延迟分析</h1><h2 id="Hadoop-的-RPC"><a href="#Hadoop-的-RPC" class="headerlink" title="Hadoop 的 RPC"></a>Hadoop 的 RPC</h2><p>RPC(Remote Procedure Call) 远程过程调用，通常采用客户端&#x2F;服务端模型，客户端像调用本地程序一样调用服务端的方法，并等待服务端响应，服务端执行完成后，把执行结果进行序列化后返回给客户端，客户端进行反序列化后继续执行后续逻辑。Hadoop 的 RPC 是自己实现的客户端和服务端，序列化反序列化使用的则是 protobuf 2.5。</p>
<h2 id="Namenode-的-RPC"><a href="#Namenode-的-RPC" class="headerlink" title="Namenode 的 RPC"></a>Namenode 的 RPC</h2><p>客户端通过执行  <code>fileComplete = dfsClient.namenode.complete(src, dfsClient.clientName, last, fileId);</code>  后，会调用NameNode 的  <code>org.apache.hadoop.hdfs.server.namenode.FSNameSystem</code> 类中的 <code>completeFile</code> 方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">boolean</span> <span class="title function_">completeFile</span><span class="params">(<span class="keyword">final</span> String src, String holder,</span></span><br><span class="line"><span class="params">                       ExtendedBlock last, <span class="type">long</span> fileId)</span></span><br><span class="line">    <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="type">boolean</span> <span class="variable">success</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line">    checkOperation(OperationCategory.WRITE);</span><br><span class="line">    <span class="keyword">final</span> <span class="type">FSPermissionChecker</span> <span class="variable">pc</span> <span class="operator">=</span> getPermissionChecker();</span><br><span class="line">    FSPermissionChecker.setOperationType(<span class="literal">null</span>);</span><br><span class="line">    writeLock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      checkOperation(OperationCategory.WRITE);</span><br><span class="line">      checkNameNodeSafeMode(<span class="string">&quot;Cannot complete file &quot;</span> + src);</span><br><span class="line">      <span class="comment">// 执行完成文件操作，会检查该文件的最小复制块是否达到要求</span></span><br><span class="line">      success = FSDirWriteFileOp.completeFile(<span class="built_in">this</span>, pc, src, holder, last,</span><br><span class="line">                                              fileId);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      writeUnlock(<span class="string">&quot;completeFile&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    getEditLog().logSync();</span><br><span class="line">    <span class="keyword">if</span> (success) &#123;</span><br><span class="line">      NameNode.stateChangeLog.info(<span class="string">&quot;DIR* completeFile: &quot;</span> + src</span><br><span class="line">          + <span class="string">&quot; is closed by &quot;</span> + holder);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> success;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>完成文件操作后调用的是  <code>org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp</code>  中的  <code>completeFileInternal</code>  方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">completeFileInternal</span><span class="params">(</span></span><br><span class="line"><span class="params">      FSNamesystem fsn, INodesInPath iip,</span></span><br><span class="line"><span class="params">      String holder, Block last, <span class="type">long</span> fileId)</span></span><br><span class="line">      <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="keyword">assert</span> fsn.hasWriteLock();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">String</span> <span class="variable">src</span> <span class="operator">=</span> iip.getPath();</span><br><span class="line">    <span class="keyword">final</span> INodeFile pendingFile;</span><br><span class="line">    <span class="type">INode</span> <span class="variable">inode</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      inode = iip.getLastINode();</span><br><span class="line">      pendingFile = fsn.checkLease(iip, holder, fileId);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (LeaseExpiredException lee) &#123;</span><br><span class="line">      <span class="keyword">if</span> (inode != <span class="literal">null</span> &amp;&amp; inode.isFile() &amp;&amp;</span><br><span class="line">          !inode.asFile().isUnderConstruction()) &#123;</span><br><span class="line">        <span class="comment">// This could be a retry RPC - i.e the client tried to close</span></span><br><span class="line">        <span class="comment">// the file, but missed the RPC response. Thus, it is trying</span></span><br><span class="line">        <span class="comment">// again to close the file. If the file still exists and</span></span><br><span class="line">        <span class="comment">// the client&#x27;s view of the last block matches the actual</span></span><br><span class="line">        <span class="comment">// last block, then we&#x27;ll treat it as a successful close.</span></span><br><span class="line">        <span class="comment">// See HDFS-3031.</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Block</span> <span class="variable">realLastBlock</span> <span class="operator">=</span> inode.asFile().getLastBlock();</span><br><span class="line">        <span class="keyword">if</span> (Block.matchingIdAndGenStamp(last, realLastBlock)) &#123;</span><br><span class="line">          NameNode.stateChangeLog.info(<span class="string">&quot;DIR* completeFile: &quot;</span> +</span><br><span class="line">              <span class="string">&quot;request from &quot;</span> + holder + <span class="string">&quot; to complete inode &quot;</span> + fileId +</span><br><span class="line">              <span class="string">&quot;(&quot;</span> + src + <span class="string">&quot;) which is already closed. But, it appears to be &quot;</span> +</span><br><span class="line">              <span class="string">&quot;an RPC retry. Returning success&quot;</span>);</span><br><span class="line">          <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">throw</span> lee;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 检查文件进度，如果文件个数小于 2 个 block 时此步骤不会生效</span></span><br><span class="line">    <span class="comment">// Check the state of the penultimate block. It should be completed</span></span><br><span class="line">    <span class="comment">// before attempting to complete the last one.</span></span><br><span class="line">    <span class="keyword">if</span> (!fsn.checkFileProgress(src, pendingFile, <span class="literal">false</span>)) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  	<span class="comment">// 此步骤进行提交和完成文件，最终会调用 BlockManger 中的 commitOrCompleteLastBlock 来更新 block 中的 uc 状态</span></span><br><span class="line">    <span class="comment">// commit the last block and complete it if it has minimum replicas</span></span><br><span class="line">    fsn.commitOrCompleteLastBlock(pendingFile, iip, last);</span><br><span class="line">		<span class="comment">// 此步骤是检查全部文件块的完成情况</span></span><br><span class="line">    <span class="keyword">if</span> (!fsn.checkFileProgress(src, pendingFile, <span class="literal">true</span>)) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    fsn.addCommittedBlocksToPending(pendingFile);</span><br><span class="line"></span><br><span class="line">    fsn.finalizeINodeFileUnderConstruction(src, pendingFile,</span><br><span class="line">        Snapshot.CURRENT_STATE_ID, <span class="literal">true</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>通过 <code>org.apache.hadoop.hdfs.server.namenode.FSNamesystem</code>  的 <code>checkFileProgress</code> 方法返回</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">// Check the state of the penultimate block. It should be completed</span></span><br><span class="line"> <span class="comment">// before attempting to complete the last one.</span></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Check that the indicated file&#x27;s blocks are present and</span></span><br><span class="line"><span class="comment"> * replicated.  If not, return false. If checkall is true, then check</span></span><br><span class="line"><span class="comment"> * all blocks, otherwise check only penultimate block.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">boolean</span> <span class="title function_">checkFileProgress</span><span class="params">(String src, INodeFile v, <span class="type">boolean</span> checkall)</span> &#123;</span><br><span class="line">  <span class="keyword">assert</span> <span class="title function_">hasReadLock</span><span class="params">()</span>;</span><br><span class="line">  <span class="keyword">if</span> (checkall) &#123;</span><br><span class="line">    <span class="keyword">return</span> checkBlocksComplete(src, <span class="literal">true</span>, v.getBlocks());</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> BlockInfo[] blocks = v.getBlocks();</span><br><span class="line">    <span class="comment">// 检查 block 长度</span></span><br><span class="line">    <span class="comment">// numCommittedAllowed 设置为 0</span></span><br><span class="line">    <span class="comment">// 也就是说，block 的长度需要大于 2 才生效</span></span><br><span class="line">    <span class="keyword">final</span> <span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> blocks.length - numCommittedAllowed - <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">return</span> i &lt; <span class="number">0</span> || blocks[i] == <span class="literal">null</span></span><br><span class="line">        || checkBlocksComplete(src, <span class="literal">false</span>, blocks[i]);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 检查块是否完成</span></span><br><span class="line"><span class="comment"> * Check if the blocks are COMPLETE;</span></span><br><span class="line"><span class="comment"> * it may allow the last block to be COMMITTED.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="type">boolean</span> <span class="title function_">checkBlocksComplete</span><span class="params">(String src, <span class="type">boolean</span> allowCommittedBlock,</span></span><br><span class="line"><span class="params">    BlockInfo... blocks)</span> &#123;</span><br><span class="line">  <span class="keyword">final</span> <span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> allowCommittedBlock? numCommittedAllowed: <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; blocks.length; i++) &#123;</span><br><span class="line">    <span class="comment">// 需要从 blockManager 获取当前 block 复制份数并检测是否达到的最小复制数</span></span><br><span class="line">    <span class="keyword">final</span> <span class="type">short</span> <span class="variable">min</span> <span class="operator">=</span> blockManager.getMinStorageNum(blocks[i]);</span><br><span class="line">    <span class="comment">// 判断 block 的 BlockUCState status 是否为 BlockUCState.COMPLETE, 如果不是，则返回错误内容</span></span><br><span class="line">    <span class="keyword">final</span> <span class="type">String</span> <span class="variable">err</span> <span class="operator">=</span> INodeFile.checkBlockComplete(blocks, i, n, min);</span><br><span class="line">    <span class="keyword">if</span> (err != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="type">int</span> <span class="variable">numNodes</span> <span class="operator">=</span> blocks[i].numNodes();</span><br><span class="line">      LOG.info(<span class="string">&quot;BLOCK* &quot;</span> + err + <span class="string">&quot;(numNodes= &quot;</span> + numNodes</span><br><span class="line">          + (numNodes &lt; min ? <span class="string">&quot; &lt; &quot;</span> : <span class="string">&quot; &gt;= &quot;</span>)</span><br><span class="line">          + <span class="string">&quot; minimum = &quot;</span> + min + <span class="string">&quot;) in file &quot;</span> + src);</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上个步骤中的 <code>fsn.commitOrCompleteLastBlock</code> 最终调用的是 <code>org.apache.hadoop.hdfs.server.blockmanagement.BlockManager </code>中的<code>commitOrCompleteLastBlock</code>方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Commit the last block of the file and mark it as complete if it has</span></span><br><span class="line"><span class="comment">   * meets the minimum redundancy requirement</span></span><br><span class="line"><span class="comment">   * </span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> bc block collection</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> commitBlock - contains client reported block length and generation</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> iip - INodes in path to bc</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@return</span> true if the last block is changed to committed state.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@throws</span> IOException if the block does not have at least a minimal number</span></span><br><span class="line"><span class="comment">   * of replicas reported from data-nodes.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">commitOrCompleteLastBlock</span><span class="params">(BlockCollection bc,</span></span><br><span class="line"><span class="params">      Block commitBlock, INodesInPath iip)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="keyword">if</span>(commitBlock == <span class="literal">null</span>)</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>; <span class="comment">// not committing, this is a block allocation retry</span></span><br><span class="line">    <span class="type">BlockInfo</span> <span class="variable">lastBlock</span> <span class="operator">=</span> bc.getLastBlock();</span><br><span class="line">    <span class="keyword">if</span>(lastBlock == <span class="literal">null</span>)</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>; <span class="comment">// no blockcommitOrCompleteLastBlocks in file yet</span></span><br><span class="line">    <span class="keyword">if</span>(lastBlock.isComplete())</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>; <span class="comment">// already completed (e.g. by syncBlock)</span></span><br><span class="line">    <span class="keyword">if</span>(lastBlock.isUnderRecovery()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IOException</span>(<span class="string">&quot;Commit or complete block &quot;</span> + commitBlock +</span><br><span class="line">          <span class="string">&quot;, whereas it is under recovery.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">committed</span> <span class="operator">=</span> commitBlock(lastBlock, commitBlock);</span><br><span class="line">    <span class="keyword">if</span> (committed &amp;&amp; lastBlock.isStriped()) &#123;</span><br><span class="line">      <span class="comment">// update scheduled size for DatanodeStorages that do not store any</span></span><br><span class="line">      <span class="comment">// internal blocks</span></span><br><span class="line">      lastBlock.getUnderConstructionFeature()</span><br><span class="line">          .updateStorageScheduledSize((BlockInfoStriped) lastBlock);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这里来判断块的复制份数</span></span><br><span class="line">    <span class="comment">// Count replicas on decommissioning nodes, as these will not be</span></span><br><span class="line">    <span class="comment">// decommissioned unless recovery/completing last block has finished</span></span><br><span class="line">    <span class="type">NumberReplicas</span> <span class="variable">numReplicas</span> <span class="operator">=</span> countNodes(lastBlock);</span><br><span class="line">    <span class="type">int</span> <span class="variable">numUsableReplicas</span> <span class="operator">=</span> numReplicas.liveReplicas() +</span><br><span class="line">        numReplicas.decommissioning() +</span><br><span class="line">        numReplicas.liveEnteringMaintenanceReplicas();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果到达最小块数，则执行 completeBlock 操作，即该块的 uc 状态改变为完成</span></span><br><span class="line">    <span class="comment">// hasMinStorage 方法会查询该 block 里的 countNodes</span></span><br><span class="line">    <span class="comment">// NumberReplicas countNodes(BlockInfo b, boolean inStartupSafeMode)</span></span><br><span class="line">    <span class="comment">// countNodes 会读取 block 里 getDataNode 中会获取 storageInfo 的信息，根据 storage 判断服务器信息</span></span><br><span class="line">    <span class="comment">// DatanodeStorageInfo storage = getStorageInfo(index);</span></span><br><span class="line">    <span class="keyword">if</span> (hasMinStorage(lastBlock, numUsableReplicas)) &#123;</span><br><span class="line">      <span class="keyword">if</span> (committed) &#123;</span><br><span class="line">        addExpectedReplicasToPending(lastBlock);</span><br><span class="line">      &#125;</span><br><span class="line">      completeBlock(lastBlock, iip, <span class="literal">false</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (pendingRecoveryBlocks.isUnderRecovery(lastBlock)) &#123;</span><br><span class="line">      <span class="comment">// We&#x27;ve just finished recovery for this block, complete</span></span><br><span class="line">      <span class="comment">// the block forcibly disregarding number of replicas.</span></span><br><span class="line">      <span class="comment">// This is to ignore minReplication, the block will be closed</span></span><br><span class="line">      <span class="comment">// and then replicated out.</span></span><br><span class="line">      completeBlock(lastBlock, iip, <span class="literal">true</span>);</span><br><span class="line">      updateNeededReconstructions(lastBlock, <span class="number">1</span>, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> committed;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p><code>INodeFile.checkBlockComplete</code> 需要判断 Block 中的状态，调用的是 <code>org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo</code> 中的 <code>getBlockUCState</code> 方法，也就是说，需要 <code>uc</code> 为空或者 <code>uc.getBlockUCState()</code>的状态就是 <code>COMPLETE</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> BlockUCState <span class="title function_">getBlockUCState</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="type">return</span> <span class="variable">uc</span> <span class="operator">=</span>= <span class="literal">null</span> ? BlockUCState.COMPLETE : uc.getBlockUCState();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>通过查询 Namenode  服务端日志：</p>
<blockquote>
<p>2022-06-10 06:22:00,480 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_3663560038_2590274987 is COMMITTED but not COMPLETE(numNodes&#x3D; 0 &lt;  minimum &#x3D; 1) in file &#x2F;path-to-file</p>
</blockquote>
<p>从此日志中可以发现，主要是因为 numNodes &#x3D; 0，但是 minimum&#x3D;1，即当前 block 中节点信息还没有被更新导致的。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>通过分析得出，之所以报错是因为  <code>Namenode</code> 发现该文件某些块没有达到最小复制数，块的状态更新在  <code>Namenode</code>  的  <code>BlockManage</code>  中，如果 <code>completeFile</code> 完成需要确保该文件的所有的 <code>block</code> 都有最小复制数。</p>
<h1 id="BlockManager-状态更新"><a href="#BlockManager-状态更新" class="headerlink" title="BlockManager 状态更新"></a>BlockManager 状态更新</h1><h2 id="HDFS-写文件"><a href="#HDFS-写文件" class="headerlink" title="HDFS 写文件"></a>HDFS 写文件</h2><p><img src="https://cdn.jsdelivr.net/gh/ft20082/image/item/hdfs-write-file.png" alt="Hadoop Write File"></p>
<h2 id="客户端写文件示例"><a href="#客户端写文件示例" class="headerlink" title="客户端写文件示例"></a>客户端写文件示例</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">writeFileToHDFS</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">  configuration.set(<span class="string">&quot;fs.defaultFS&quot;</span>, <span class="string">&quot;hdfs://localhost:9000&quot;</span>);</span><br><span class="line">  configuration.addResource(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/HADOOP_HOME/conf/core-site.xml&quot;</span>));</span><br><span class="line">  configuration.addResource(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/HADOOP_HOME/conf/hdfs-site.xml&quot;</span>));</span><br><span class="line">  <span class="type">FileSystem</span> <span class="variable">fileSystem</span> <span class="operator">=</span> FileSystem.get(configuration);</span><br><span class="line">  <span class="comment">//Create a path</span></span><br><span class="line">  <span class="type">String</span> <span class="variable">fileName</span> <span class="operator">=</span> <span class="string">&quot;input.txt&quot;</span>;</span><br><span class="line">  <span class="type">Path</span> <span class="variable">hdfsWritePath</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/user/yourdesiredpath/&quot;</span> + fileName);</span><br><span class="line">  <span class="type">FSDataOutputStream</span> <span class="variable">fsDataOutputStream</span> <span class="operator">=</span> fileSystem.create(hdfsWritePath,<span class="literal">true</span>);</span><br><span class="line">  fsDataOutputStream.writeBytes(<span class="string">&quot;write contents&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">  fsDataOutputStream.close();</span><br><span class="line">  fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>示例中，首先要创建一个 FileSystem 实例，根据配置文件，实例化  <code>org.apache.hadoop.fs.Hdfs</code>，同时会初始化 <code>org.apache.hadoop.hdfs.DFSClient</code>，在调用  <code>create</code> 时实际调用的是  <code>dfs.createWrappedOutputStream</code> 返回 <code>DFSOutputStream</code>，使用  <code>writeBytes</code> 写入文件时，以  <code>DFSPacket</code>  逐个由 <code>DataStreamer</code> 发送。</p>
<h2 id="DanaNode-流式写文件"><a href="#DanaNode-流式写文件" class="headerlink" title="DanaNode 流式写文件"></a>DanaNode 流式写文件</h2><p>数据在多个节点传输时，<code>DFSOutputStream</code>  写入一个 <code> PIPELINE</code>，即由 1 个客户端和多个 <code>DataNode</code> 连接起来的数据通道，当客户端开始写文件时，会实时写进第一个节点，再写到第二个节点，依次类推写到最后一个节点。写入完成后，会返回 <code>PipelineAck</code> 确认。<code>DFSOutputStream</code> 维护一个  <code>DFSPacket</code> 队列  <code>dataQueue</code>， 当数据写入队列后，<code>DFSOutputStream</code> 先申请一个块，并创建 <code>PIPELINE</code>，数据写入，当达到块的最大数据量时，会先关闭这个块，并关闭相关连接。继续写入数据时会重新申请一个块，<code>DataStreamer</code> 调用  <code>createBlockOutputStream</code>  重新创建 <code>PIPELINE</code>  并写入数据，直至文件写入完成。</p>
<p>数据传输协议  <code>DataTransferProtocol</code>  有 2 个实现，一是发送端  <code>org.apache.hadoop.hdfs.protocol.datatransfer.Sender</code> ，另外是接收端  <code>org.apache.hadoop.hdfs.server.datanode.DataXceiver</code>。 其中发送是在客户端和 <code>Datanode</code> 里都有使用，而接收端只有在 <code>Datanode</code> 里使用。</p>
<h2 id="DataNode-数据写入流程"><a href="#DataNode-数据写入流程" class="headerlink" title="DataNode 数据写入流程"></a>DataNode 数据写入流程</h2><ol>
<li>客户端向  <code>Namenode</code>  发送写入文件请求，在  <code>org.apache.hadoop.hdfs.DFSOutputStream </code>  里的方法  <code>newStreamForCreate</code>  调用  <code>dfsClient.namenode.create</code>  创建写入文件并返回给客户端</li>
<li>客户端创建  <code>DFSOutputStream</code>  写入数据，该实例会创建一个  <code>org.apache.hadoop.hdfs.DataStreamer</code>  线程准备写入数据。 </li>
<li><code>FSDataOutputStream</code>  写入的数据时会被组装成  <code>DFSPacket</code>   放入到  <code>DataStreamer</code>  里的  <code>dataQueue</code>  中，线程 <code>DataStreamer</code> 从队列里读取  <code>DFSPacket</code>  并发送到   <code>PIPELINE</code>  里。如果管道没有初始化，则通过  <code>org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream</code>  里调用  <code>DFSOutputStream.addBlock</code>  往该文件添加 Block 返回地址并初始化建立连接，使用  <code>org.apache.hadoop.hdfs.protocol.datatransfer.Sender</code>  中的  <code>writeBlock</code>  方法发送创建连接后的初始验证数据包  <code>OpWriteBlockProto</code> </li>
<li>当  <code>DataStreamer</code> 创建连接时，连接到  <code>DataNode</code>  中的  <code>org.apache.hadoop.hdfs.server.datanode.DataXceiver</code>，<code>DataNode</code>       为每个写入连接分配一个线程，并建立连接。在  <code>DataXceiver</code>  读取到  <code>Sender</code>  发送的  <code>OpWriteBlockProto</code>  后进行校验，校验通过，则调用  <code>writeBlock</code>  方法， 该方法会根据 targets 选择是否开启下一  <code>DataNode</code>  连接，往下个节点发送也使用的是  <code>Sender</code> 类的  <code>writeBlock</code> 方法，依次类推，如果是 3 副本，则会重复 3 次此操作。调用  <code>writeBlock</code>  的时候会把 targets  数组的第一个节点丢弃，所以会依次连接 targes 列表中所有节点，通过  <code>setCurrentBlockReceiver</code>  方法设置接收 Block 的对象  <code>org.apache.hadoop.hdfs.server.datanode.BlockReceiver</code>，在调用  <code>BlockReceiver.receiveBlock</code>  后开始接收  <code>DFSPacket</code>  数据，<code>receiveBlock</code>  方法会起一个  <code>PacketResponder</code>  线程来发送返回的  <code>Packet</code> 。</li>
<li><code>BlockReceiver</code>  在初始化的时候，会创建  <code>ReplicaHandler</code>，在初始写文件时，即状态为  <code>PIPELINE_SETUP_CREATE</code>  状态，同时调用 RPC  <code>datanode.notifyNamenodeReceivingBlock</code>  发送消息给  <code>NameNode</code> ，通知正在接收 Block，当 BlockStatus 为  <code>BlockStatus.RECEIVING_BLOCK</code>  时，会等到下次心跳的时候统一发送，如果是状态是  <code>BlockStatus.RECEIVED_BLOCK</code>  时，则会立即发送。最终会使用 RPC 调用 <code>NameNode</code> 的  <code>namenode.blockReceivedAndDeleted</code>  方法实现。</li>
<li><code>BlockReceiver</code>  中  <code>receivePacket</code>  方法会一直执行收到最后一个  <code>DFSPacket</code>，即  <code>PacketHeader</code>   通过  <code>isLastPacketInBlock</code> 来判断是否是该 Block 最后一个 <code>DFSPacket</code>， 每接收到一个  <code>DFSPacket</code>，如果存在下个节点，则先把数据写入到下个节点，然后通过  <code>streams.writeDataToDisk</code>  和  <code>checksumOut.write</code>  写入本地磁盘，磁盘上会有 2 个文件，一个是数据文件，一个是 checkSum。写入完成后，往  <code>PacketResponder</code>  里的  <code>ackQueue</code>  里写入一个 <code>Packet</code> 主要信息有当前包的序号、是否是当前 <em>Block</em> 的最后一个，Block 的位置，系统时间，ack状态。</li>
<li><code>PacketResponder</code> 收到了最后一个 <code>Packet</code> 后，则开始执行 <code>finalizeBlock</code> 方法，该方法完成 Block 的写入后会执行 <code>datanode.closeBlock</code> 方法，即调用 <code>DataNode.notifyNamenodeReceivedBlock</code> 方法上报已经收到的块。如果状态是 <code>BlockStatus.RECEIVED_BLOCK</code> 时，则会立即通过 RPC 发送给  <code>NameNode</code> </li>
<li>当一个  <code>DFSPacket</code>   被发送到每个节点时， <code>BlockReceiver</code>  类中的  <code>PacketResponder</code>  内部类会根据是否是最后一个节点来决定往上游写入流返回数据  <code>org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck</code>  。<ul>
<li>如果是最后一个节点，则会根据  <code>DFSPacket</code>  的数据写入、校验情况决定是否返回成功或者校验失败</li>
<li>如果是中间节点，则优先接收下游节点返回的 <code>PipelineAck</code> 并判断状态，如果正常，则添加当前节点的状态后返回会上游</li>
</ul>
</li>
<li>客户端的 <code>DataStreamer</code>  在每次创建一个新的 block 时，通过  <code>initDataStreaming</code>  方法初始化  <code>DataStreamer</code>  类，并初始化  <code>ResponseProcessor</code>  线程，该线程的作用接收  <code>DataNode</code>  的返回  <code>PipelineAck</code>，检查并校验该  <code>DFSPacket</code>  是否写入成功，如果不成功，则抛出 IO 异常。如果是该 Block 的最后一个 Packet，则验证完成后退出。</li>
<li>当写入一个文件比较大需要写入多个 Block 时，会重复执行获取 Block、创建连接、发送 <code>DFSPacket</code> 、校验返回等操作直至写入完成。</li>
</ol>
<h2 id="BlockManager-的块更新"><a href="#BlockManager-的块更新" class="headerlink" title="BlockManager 的块更新"></a>BlockManager 的块更新</h2><p> <code>DataNode</code>  接收到的 Block 写入完成后， <code>BlockReceiver</code>  中的  <code>PacketResponder</code>  会调用  <code>DataNode.notifyNamenodeReceivedBlock</code>  方法，该方法会把块信息写入到  <code>org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager</code>  里，由线程   <code>IBRTaskHandler</code>  根据条件定时还是即时上报到   <code>NameNode</code>，最终在  <code>DataNode</code>  中调用的是 RPC  <code>namenode.blockReceivedAndDeleted</code>  方法通知  <code>NameNode</code>。</p>
<p><code>NameNode</code> 的 <code>org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer</code> 通过方法 <code>blockReceivedAndDeleted</code> 接收到 rpc 请求，获取 <code>BlockManager</code> 实例 <code>bm</code>，把每一个 <code>StorageReceivedDeletedBlocks</code> 对象以线程形式放入到 <code>BlockReportProcessingThread</code> 的队列中。</p>
<p><img src="https://cdn.jsdelivr.net/gh/ft20082/image/item/block_incremental_manager_data_flow.png" alt="block incremental manager data flow"></p>
<blockquote>
<p>DataNode 通过增量更新 NameNode 的 Block 的过程中，以批量形式发送，即在接收到 Receiving 状态的块时是不会即时通知 NameNode 的，只有是 Received 状态的更新会即时发送。 </p>
</blockquote>
<p><code>BlockManager.BlockReportProcessingThread</code> 线程会一直从队列里获取任务并执行，当达到最大的持有锁时间时，会退出重新获取锁，防止持有太长时间导致其他写操作超时。<code>BlockReportProcessingThread</code> 执行逻辑</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">processQueue</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="keyword">while</span> (namesystem.isRunning()) &#123;</span><br><span class="line">    <span class="type">NameNodeMetrics</span> <span class="variable">metrics</span> <span class="operator">=</span> NameNode.getNameNodeMetrics();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 从队列中获取任务</span></span><br><span class="line">      <span class="type">Runnable</span> <span class="variable">action</span> <span class="operator">=</span> queue.take();</span><br><span class="line">      <span class="comment">// batch as many operations in the write lock until the queue</span></span><br><span class="line">      <span class="comment">// runs dry, or the max lock hold is reached.</span></span><br><span class="line">      <span class="type">int</span> <span class="variable">processed</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">      namesystem.writeLock();</span><br><span class="line">      <span class="comment">// 可以通过 metrics 添加，可以通过 http://ip:port/jmx 查看</span></span><br><span class="line">      metrics.setBlockOpsQueued(queue.size() + <span class="number">1</span>);</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span> Time.monotonicNow();</span><br><span class="line">        <span class="keyword">do</span> &#123;</span><br><span class="line">          processed++;</span><br><span class="line">          <span class="comment">// 该操作执行的是 BlockManager.processIncrementalBlockReport(DatanodeID nodeID, StorageReceivedDeletedBlocks srdb);</span></span><br><span class="line">          <span class="comment">// 根据类型判断执行  removeStoredBlock / addBlock / processAndHandleReportedBlock</span></span><br><span class="line">          action.run();</span><br><span class="line">          <span class="comment">// 如果超过最大的持有锁的时间，则退出</span></span><br><span class="line">          <span class="comment">// maxLockHoldTime 默认值是 4ms</span></span><br><span class="line">          <span class="keyword">if</span> (Time.monotonicNow() - start &gt; maxLockHoldTime) &#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">          &#125;</span><br><span class="line">          action = queue.poll();</span><br><span class="line">        &#125; <span class="keyword">while</span> (action != <span class="literal">null</span>);</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        namesystem.writeUnlock(<span class="string">&quot;processQueue&quot;</span>);</span><br><span class="line">        metrics.addBlockOpsBatched(processed - <span class="number">1</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">      <span class="comment">// ignore unless thread was specifically interrupted.</span></span><br><span class="line">      <span class="keyword">if</span> (Thread.interrupted()) &#123;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  queue.clear();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="NameNode-RPC-blockReceivedAndDeleted"><a href="#NameNode-RPC-blockReceivedAndDeleted" class="headerlink" title="NameNode RPC blockReceivedAndDeleted"></a>NameNode RPC blockReceivedAndDeleted</h3><p><code>BlockReportProcessingThread</code> 执行内容是 <code>namesystem.processIncrementalBlockReport(nodeReg, r);</code> ，该方法会调用 <code>blockManager.processIncrementalBlockReport(nodeID, srdb)</code> ，最终会根据类型来判断执行 <code>removeStoredBlock</code>、 <code>addBlock</code>、 <code>processAndHandleReportedBlock</code> 。</p>
<h4 id="removeStoredBlock-移除Block"><a href="#removeStoredBlock-移除Block" class="headerlink" title="removeStoredBlock 移除Block"></a>removeStoredBlock 移除Block</h4><ol>
<li><code>BlockMap</code>  和  <code>DatanodeStorageInfo</code>  移除 Block信息</li>
<li>清理 CacheManager 里的 Block 信息</li>
</ol>
<h4 id="addBlock-添加-Block"><a href="#addBlock-添加-Block" class="headerlink" title="addBlock 添加 Block"></a>addBlock 添加 Block</h4><ol>
<li>移除  <code>BlockManager.pendingReconstruction</code>  里的 Block</li>
<li>执行  <code>processAndHandleReportedBlock</code></li>
</ol>
<h4 id="processAndHandleReportedBlock-处理上报-Block"><a href="#processAndHandleReportedBlock-处理上报-Block" class="headerlink" title="processAndHandleReportedBlock 处理上报 Block"></a>processAndHandleReportedBlock 处理上报 Block</h4><ol>
<li>如果块查找不到，则标记为  <code>invalidate</code></li>
<li>如果上报副本有效，和  <code>NameNode</code>  有相同的时间戳和长度，则执行添加 Block  <code>addStoredBlock</code>  操作</li>
<li>如果上报副本无效，则标记为  <code>corrupt</code>，<code>corrupt</code> 块在 Block 完成复制后会被删除</li>
<li>如果上报副本状态在  <code>NameNode</code>  中为  <code>under construction</code>，将被添加到  <code>BlockUnderConstructionFeature</code>  列表里</li>
</ol>
<h3 id="添加存储-Block-操作-addStoredBlock"><a href="#添加存储-Block-操作-addStoredBlock" class="headerlink" title="添加存储 Block 操作 addStoredBlock"></a>添加存储 Block 操作 addStoredBlock</h3><p>修改 <code>(block -&gt; datanode)</code> 映射，从 <code>reconstruction</code> 里移除 block</p>
<ol>
<li>检查 block 是否存在或者是否删除，如果不存在或者已删除，则忽略</li>
<li>添加往  <code>DatanodeStorageInfo</code>  映射里添加 Block</li>
<li>计算 block 的副本数，如果 storedBlock 的状态是  <code>BlockUCState.COMMITTED</code>  且已经达到了最小副本 ，则执行  <code>completeBlock</code>，该方法最终会调用  <code>convertToCompleteBlock</code>  来修改 Block 的 uc 状态，即 <code>DFSClient</code> 在客户端调用 <code>completeFile</code> 时检查的位置 </li>
<li>检查冗余数量，如果不需要冗余，则从  <code>neededReconstruction</code>  里删除，如果超过，则执行  <code>invalidateCorruptReplicas</code> ，把 Block 添加到  <code>corruptReplicas</code>  清理冗余。</li>
</ol>
<h2 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h2><p>在 HDFS 客户端写文件的完成后，Block 中的数据节点更新是由 danaNode 节点通过RPC  <code>rpc blockReceivedAndDeleted(BlockReceivedAndDeletedRequestProto)      returns(BlockReceivedAndDeletedResponseProto);</code>  上报到 Namenode 节点，<code>NameNode</code>  把此操作封装成一个 RPC Call 异步执行更新  <code>BlockManage</code>  中  <code>BlockMap</code>  里 <code>BlockInfo</code> 对象。同时修改  <code>DatanodeStorageInfo</code>  里的 Block 信息，执行状态更新是由一个独立的线程 <code>BlockReportProcessingThread</code> 一直执行，该线程在获取锁的最大时间，默认 4 毫秒后，会释放锁，并准备重新获取写入锁并执行。</p>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p><code>DFSClient</code> 调用 <code>completeFile</code> 失败的原因是因为 <code>DataNode</code> 节点虽然通过 RPC <code>blockReceivedAndDeleted</code> 把 Block  的状态 <code>BlockStatus.RECEIVED_BLOCK</code>  上报给 NameNode，由于 NameNode 在处理这个响应的过程中采用异步策略，导致了 BlockManager 处理队列里的内容延迟，没有及时更新 Block 的状态。重试多次查询后依然失败，导致错误发生。</p>
<h1 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h1><ol>
<li>优先优化作业，查询是否有占用资源比较大且小文件很多的作业</li>
<li>可以适当增加重试次数，设置为 6 或 7 次，增加成功的概率</li>
<li>优化 RPC 队列处理时间和队列时间，解决 RPC 延迟问题</li>
</ol>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>配置带安全认证的 Kafka + zk 集群(SASL)</title>
    <url>/posts/3578056852/</url>
    <content><![CDATA[<h1 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h1><h2 id="主机信息"><a href="#主机信息" class="headerlink" title="主机信息"></a>主机信息</h2><ul>
<li>192.168.1.100</li>
<li>192.168.1.101</li>
<li>192.168.1.102</li>
</ul>
<h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><ul>
<li><p>Zookeeper 版本：apache-zookeeper-3.6.3</p>
</li>
<li><p>Kafka 版本：kafka_2.12-2.3.1</p>
</li>
<li><p>Centos 版本：CentOS Linux release 7.9.2009 (Core)</p>
</li>
<li><p>Java 版本：openjdk version “1.8.0_275”</p>
</li>
</ul>
<h1 id="Zookeeper-安装"><a href="#Zookeeper-安装" class="headerlink" title="Zookeeper 安装"></a>Zookeeper 安装</h1><p>Zookeeper 从 3.5.9 &#x2F; 3.6.0 开始支持 SASL 安全认证，ZK 的认证有 2 方面，一是客户端到服务端，另外一个是服务端到服务端，每种方式开启的也不一样。</p>
<h2 id="客户端-服务端-SASL-认证"><a href="#客户端-服务端-SASL-认证" class="headerlink" title="客户端-服务端 SASL 认证"></a>客户端-服务端 SASL 认证</h2><p>客户端-服务端认证是 zk 客户端通过 SASL 认证访问服务端，如果开启，需要在 zoo.cfg 配置文件里加入如下的配置项</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">authProvider.1</span>=org.apache.zookeeper.server.auth.SASLAuthenticationProvider</span><br><span class="line"><span class="comment"># 此项必须配置</span></span><br><span class="line"><span class="attr">sessionRequireClientSASLAuth</span>=<span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h2 id="服务端-服务端-SASL-认证"><a href="#服务端-服务端-SASL-认证" class="headerlink" title="服务端-服务端 SASL 认证"></a>服务端-服务端 SASL 认证</h2><p>服务端认证支持 Kerberos 和 JAAS，统一使用了 JAAS</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">quorum.auth.enableSasl</span>=<span class="literal">true</span></span><br><span class="line"><span class="attr">quorum.auth.learnerRequireSasl</span>=<span class="literal">true</span></span><br><span class="line"><span class="attr">quorum.auth.serverRequireSasl</span>=<span class="literal">true</span></span><br><span class="line"><span class="attr">quorum.auth.learner.saslLoginContext</span>=QuorumLearner</span><br><span class="line"><span class="attr">quorum.auth.server.saslLoginContext</span>=QuorumServer</span><br><span class="line"><span class="attr">quorum.auth.kerberos.servicePrincipal</span>=servicename/_HOST</span><br><span class="line"><span class="attr">quorum.cnxn.threads.size</span>=<span class="number">20</span></span><br></pre></td></tr></table></figure>

<h2 id="Zookeeper-配置文件-zoo-cfg"><a href="#Zookeeper-配置文件-zoo-cfg" class="headerlink" title="Zookeeper 配置文件 zoo.cfg"></a>Zookeeper 配置文件 zoo.cfg</h2><figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The number of milliseconds of each tick</span></span><br><span class="line"><span class="attr">tickTime</span>=<span class="number">2000</span></span><br><span class="line"><span class="comment"># The number of ticks that the initial </span></span><br><span class="line"><span class="comment"># synchronization phase can take</span></span><br><span class="line"><span class="attr">initLimit</span>=<span class="number">10</span></span><br><span class="line"><span class="comment"># The number of ticks that can pass between </span></span><br><span class="line"><span class="comment"># sending a request and getting an acknowledgement</span></span><br><span class="line"><span class="attr">syncLimit</span>=<span class="number">5</span></span><br><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just </span></span><br><span class="line"><span class="comment"># example sakes.</span></span><br><span class="line"><span class="attr">dataDir</span>=/opt/developer/kafka/apache-zookeeper-<span class="number">3.6</span>.<span class="number">3</span>-bin/data/</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line"><span class="attr">clientPort</span>=<span class="number">2181</span></span><br><span class="line"><span class="comment"># the maximum number of client connections.</span></span><br><span class="line"><span class="comment"># increase this if you need to handle more clients</span></span><br><span class="line"><span class="comment">#maxClientCnxns=60</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Be sure to read the maintenance section of the </span></span><br><span class="line"><span class="comment"># administrator guide before turning on autopurge.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The number of snapshots to retain in dataDir</span></span><br><span class="line"><span class="comment">#autopurge.snapRetainCount=3</span></span><br><span class="line"><span class="comment"># Purge task interval in hours</span></span><br><span class="line"><span class="comment"># Set to &quot;0&quot; to disable auto purge feature</span></span><br><span class="line"><span class="comment">#autopurge.purgeInterval=1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Metrics Providers</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># https://prometheus.io Metrics Exporter</span></span><br><span class="line"><span class="comment">#metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider</span></span><br><span class="line"><span class="comment">#metricsProvider.httpPort=7000</span></span><br><span class="line"><span class="comment">#metricsProvider.exportJvmInfo=true</span></span><br><span class="line"><span class="comment"># 服务器配置信息</span></span><br><span class="line"><span class="attr">server.1</span>=<span class="number">192.168</span>.<span class="number">1.100</span>:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line"><span class="attr">server.2</span>=<span class="number">192.168</span>.<span class="number">1.101</span>:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line"><span class="attr">server.3</span>=<span class="number">192.168</span>.<span class="number">1.102</span>:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line"><span class="comment"># 客户端 SASL 配置项</span></span><br><span class="line"><span class="attr">authProvider.1</span>=org.apache.zookeeper.server.auth.SASLAuthenticationProvider</span><br><span class="line"><span class="attr">sessionRequireClientSASLAuth</span>=<span class="literal">true</span></span><br><span class="line"><span class="comment"># 服务端 SASL 配置项</span></span><br><span class="line"><span class="attr">quorum.auth.enableSasl</span>=<span class="literal">true</span></span><br><span class="line"><span class="attr">quorum.auth.learnerRequireSasl</span>=<span class="literal">true</span></span><br><span class="line"><span class="attr">quorum.auth.serverRequireSasl</span>=<span class="literal">true</span></span><br><span class="line"><span class="attr">quorum.auth.learner.saslLoginContext</span>=QuorumLearner</span><br><span class="line"><span class="attr">quorum.auth.server.saslLoginContext</span>=QuorumServer</span><br><span class="line"><span class="attr">quorum.cnxn.threads.size</span>=<span class="number">20</span></span><br></pre></td></tr></table></figure>

<h2 id="设置-Jaas-conf-配置"><a href="#设置-Jaas-conf-配置" class="headerlink" title="设置 Jaas.conf 配置"></a>设置 Jaas.conf 配置</h2><p>在 conf 目录添加 Jaas.conf 配置文件，服务端和和客户端的 Jaas 单独配置</p>
<h3 id="服务端-Jaas-conf"><a href="#服务端-Jaas-conf" class="headerlink" title="服务端 Jaas.conf"></a>服务端 Jaas.conf</h3><figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">QuorumServer &#123;</span><br><span class="line">       org.apache.zookeeper.server.auth.DigestLoginModule required</span><br><span class="line">       <span class="attr">user_zookeeper</span>=<span class="string">&quot;admin-zookeeper&quot;</span><span class="comment">;</span></span><br><span class="line">&#125;<span class="comment">;</span></span><br><span class="line"> </span><br><span class="line">QuorumLearner &#123;</span><br><span class="line">       org.apache.zookeeper.server.auth.DigestLoginModule required</span><br><span class="line">       <span class="attr">username</span>=<span class="string">&quot;zookeeper&quot;</span></span><br><span class="line">       <span class="attr">password</span>=<span class="string">&quot;admin-zookeeper&quot;</span><span class="comment">;</span></span><br><span class="line">&#125;<span class="comment">;</span></span><br><span class="line"></span><br><span class="line">Server &#123;</span><br><span class="line">       org.apache.zookeeper.server.auth.DigestLoginModule required</span><br><span class="line">       <span class="attr">user_super</span>=<span class="string">&quot;admin-zookeeper&quot;</span></span><br><span class="line">       <span class="attr">user_zookeeper</span>=<span class="string">&quot;client-zookeeper&quot;</span><span class="comment">;</span></span><br><span class="line">&#125;<span class="comment">;</span></span><br></pre></td></tr></table></figure>

<h3 id="客户端-Jaas-client-conf"><a href="#客户端-Jaas-client-conf" class="headerlink" title="客户端 Jaas-client.conf"></a>客户端 Jaas-client.conf</h3><figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">Client &#123;</span><br><span class="line">       org.apache.zookeeper.server.auth.DigestLoginModule required</span><br><span class="line">       <span class="attr">username</span>=<span class="string">&quot;zookeeper&quot;</span></span><br><span class="line">       <span class="attr">password</span>=<span class="string">&quot;client-zookeeper&quot;</span><span class="comment">;</span></span><br><span class="line">&#125;<span class="comment">;</span></span><br></pre></td></tr></table></figure>

<h2 id="设置-ZK-启动环境变量-java-env"><a href="#设置-ZK-启动环境变量-java-env" class="headerlink" title="设置 ZK 启动环境变量 java.env"></a>设置 ZK 启动环境变量 java.env</h2><p>在 conf 目录环境添加文件 java.env 设置 Java 启动环境</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">SERVER_JVMFLAGS=&quot;-Djava.security.auth.login.config=/opt/developer/kafka/apache-zookeeper-3.6.3-bin/conf/jaas.conf&quot;</span><br><span class="line">CLIENT_JVMFLAGS=&quot;$&#123;CLIENT_JVMFLAGS&#125; -Djava.security.auth.login.config=/opt/developer/kafka/apache-zookeeper-3.6.3-bin/conf/jaas-client.conf&quot;</span><br></pre></td></tr></table></figure>

<h2 id="配置目录"><a href="#配置目录" class="headerlink" title="配置目录"></a>配置目录</h2><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">/opt/developer/kafka/apache-zookeeper-3.6.3-bin/conf/</span><br><span class="line">├── configuration.xsl</span><br><span class="line">├── jaas-client.conf</span><br><span class="line">├── jaas.conf</span><br><span class="line">├── java.env</span><br><span class="line">├── log4j.properties</span><br><span class="line">├── zoo.cfg</span><br><span class="line">└── zoo_sample.cfg</span><br></pre></td></tr></table></figure>

<h2 id="配置节点-ID"><a href="#配置节点-ID" class="headerlink" title="配置节点 ID"></a>配置节点 ID</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">192.168.1.100 节点设置</span></span><br><span class="line">echo &quot;1&quot; &gt; /opt/developer/kafka/apache-zookeeper-3.6.3-bin/data/myid</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">192.168.1.101 节点设置</span></span><br><span class="line">echo &quot;2&quot; &gt; /opt/developer/kafka/apache-zookeeper-3.6.3-bin/data/myid</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">192.168.1.102 节点设置</span></span><br><span class="line">echo &quot;3&quot; &gt; /opt/developer/kafka/apache-zookeeper-3.6.3-bin/data/myid</span><br></pre></td></tr></table></figure>

<h2 id="启动-Zookeeper"><a href="#启动-Zookeeper" class="headerlink" title="启动 Zookeeper"></a>启动 Zookeeper</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动服务</span></span><br><span class="line">./bin/zkServer.sh start</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看状态</span></span><br><span class="line">./bin/zkServer.sh status</span><br></pre></td></tr></table></figure>

<h2 id="Zookeeper-客户端使用"><a href="#Zookeeper-客户端使用" class="headerlink" title="Zookeeper 客户端使用"></a>Zookeeper 客户端使用</h2><p>客户端需要使用 JAAS，统一放在 jaas.conf。ZK 启动后，也会加载 conf&#x2F;java.env</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh ./bin/zkCli.sh -server 192.168.1.102:2181</span><br></pre></td></tr></table></figure>

<h1 id="Kafka-安装"><a href="#Kafka-安装" class="headerlink" title="Kafka 安装"></a>Kafka 安装</h1><p>Kafka SASL 认证也有 2 方面， 一是 连接 ZK 使用 SASL，另外一个是 Kafka 客户端使用 SCRAM 连接 Kafka broker。</p>
<h2 id="Kafka-连接-ZK-SASL-认证"><a href="#Kafka-连接-ZK-SASL-认证" class="headerlink" title="Kafka 连接 ZK SASL 认证"></a>Kafka 连接 ZK SASL 认证</h2><p>Kafka 连接 zk 使用 JAAS，添加 config&#x2F;kafka-server-jaas.conf</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">Client &#123;</span><br><span class="line">       org.apache.zookeeper.server.auth.DigestLoginModule required</span><br><span class="line">       <span class="attr">username</span>=<span class="string">&quot;zookeeper&quot;</span></span><br><span class="line">       <span class="attr">password</span>=<span class="string">&quot;client-zookeeper&quot;</span><span class="comment">;</span></span><br><span class="line">&#125;<span class="comment">;</span></span><br></pre></td></tr></table></figure>

<p>在使用 Kafka 命令时，需要把改配置文件放到Kafka 的环境变量里</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/developer/kafka/kafka_2.12-2.3.1/config/kafka-server-jaas.conf&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看 topic list</span></span><br><span class="line">sh ./bin/kafka-topics.sh --zookeeper 192.168.1.100:2181  --list</span><br></pre></td></tr></table></figure>



<h2 id="Kafka-服务端开启-SCRAM"><a href="#Kafka-服务端开启-SCRAM" class="headerlink" title="Kafka 服务端开启 SCRAM"></a>Kafka 服务端开启 SCRAM</h2><h3 id="添加认证权限"><a href="#添加认证权限" class="headerlink" title="添加认证权限"></a>添加认证权限</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置环境变量</span></span><br><span class="line">export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/developer/kafka/kafka_2.12-2.3.1/config/kafka-server-jaas.conf&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加认证用户</span></span><br><span class="line">sh ./bin/kafka-configs.sh --zookeeper 192.168.1.100:2181 --alter --add-config &#x27;SCRAM-SHA-256=[iterations=8192,password=kafka-user],SCRAM-SHA-512=[password=kafka-user]&#x27; --entity-type users --entity-name kakfa-user</span><br><span class="line">Completed Updating config for entity: user-principal &#x27;kafka-user&#x27;.</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加认证用户</span></span><br><span class="line">sh ./bin/kafka-configs.sh --zookeeper 192.168.1.100:2181 --alter --add-config &#x27;SCRAM-SHA-256=[iterations=8192,password=kafka-admin],SCRAM-SHA-512=[password=kafka-admin]&#x27; --entity-type users --entity-name kakfa-admin         </span><br><span class="line">Completed Updating config for entity: user-principal &#x27;kafka-admin&#x27;.</span><br></pre></td></tr></table></figure>

<h3 id="添加-Broker-配置"><a href="#添加-Broker-配置" class="headerlink" title="添加 Broker 配置"></a>添加 Broker 配置</h3><p>Kafka 服务端认证，添加以下内容到 config&#x2F;kafka-server-jaas.conf</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">KafkaServer &#123;</span><br><span class="line">    org.apache.kafka.common.security.scram.ScramLoginModule required</span><br><span class="line">    <span class="attr">username</span>=<span class="string">&quot;kakfa-admin&quot;</span></span><br><span class="line">    <span class="attr">password</span>=<span class="string">&quot;kafka-admin&quot;</span><span class="comment">;</span></span><br><span class="line">&#125;<span class="comment">;</span></span><br></pre></td></tr></table></figure>

<p>服务端配置项</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">############################# Server Basics #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The id of the broker. This must be set to a unique integer for each broker.</span></span><br><span class="line"><span class="attr">broker.id</span>=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">############################# Socket Server Settings #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The address the socket server listens on. It will get the value returned from </span></span><br><span class="line"><span class="comment"># java.net.InetAddress.getCanonicalHostName() if not configured.</span></span><br><span class="line"><span class="comment">#   FORMAT:</span></span><br><span class="line"><span class="comment">#     listeners = listener_name://host_name:port</span></span><br><span class="line"><span class="comment">#   EXAMPLE:</span></span><br><span class="line"><span class="comment">#     listeners = PLAINTEXT://your.host.name:9092</span></span><br><span class="line"><span class="attr">listeners</span>=SASL_PLAINTEXT://<span class="number">192.168</span>.<span class="number">1.100</span>:<span class="number">9092</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hostname and port the broker will advertise to producers and consumers. If not set, </span></span><br><span class="line"><span class="comment"># it uses the value for &quot;listeners&quot; if configured.  Otherwise, it will use the value</span></span><br><span class="line"><span class="comment"># returned from java.net.InetAddress.getCanonicalHostName().</span></span><br><span class="line"><span class="comment"># 如果不开启外网，则不需要配置</span></span><br><span class="line"><span class="comment"># advertised.listeners=SASL_PLAINTEXT://192.168.1.100:9092</span></span><br><span class="line"><span class="comment"># 安全认证方式，如果开启则需要添加此项</span></span><br><span class="line"><span class="attr">security.inter.broker.protocol</span>=SASL_PLAINTEXT</span><br><span class="line"></span><br><span class="line"><span class="comment"># Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details</span></span><br><span class="line"><span class="comment">#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The number of threads that the server uses for receiving requests from the network and sending responses to the network</span></span><br><span class="line"><span class="attr">num.network.threads</span>=<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The number of threads that the server uses for processing requests, which may include disk I/O</span></span><br><span class="line"><span class="attr">num.io.threads</span>=<span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The send buffer (SO_SNDBUF) used by the socket server</span></span><br><span class="line"><span class="attr">socket.send.buffer.bytes</span>=<span class="number">102400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The receive buffer (SO_RCVBUF) used by the socket server</span></span><br><span class="line"><span class="attr">socket.receive.buffer.bytes</span>=<span class="number">102400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The maximum size of a request that the socket server will accept (protection against OOM)</span></span><br><span class="line"><span class="attr">socket.request.max.bytes</span>=<span class="number">104857600</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">############################# Log Basics #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A comma separated list of directories under which to store log files</span></span><br><span class="line"><span class="attr">log.dirs</span>=/opt/developer/kafka/kafka_2.<span class="number">12</span>-<span class="number">2.3</span>.<span class="number">1</span>/data</span><br><span class="line"></span><br><span class="line"><span class="comment"># The default number of log partitions per topic. More partitions allow greater</span></span><br><span class="line"><span class="comment"># parallelism for consumption, but this will also result in more files across</span></span><br><span class="line"><span class="comment"># the brokers.</span></span><br><span class="line"><span class="attr">num.partitions</span>=<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.</span></span><br><span class="line"><span class="comment"># This value is recommended to be increased for installations with data dirs located in RAID array.</span></span><br><span class="line"><span class="attr">num.recovery.threads.per.data.dir</span>=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">############################# Internal Topic Settings  #############################</span></span><br><span class="line"><span class="comment"># The replication factor for the group metadata internal topics &quot;__consumer_offsets&quot; and &quot;__transaction_state&quot;</span></span><br><span class="line"><span class="comment"># For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.</span></span><br><span class="line"><span class="attr">offsets.topic.replication.factor</span>=<span class="number">1</span></span><br><span class="line"><span class="attr">transaction.state.log.replication.factor</span>=<span class="number">1</span></span><br><span class="line"><span class="attr">transaction.state.log.min.isr</span>=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">############################# Log Retention Policy #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The minimum age of a log file to be eligible for deletion due to age</span></span><br><span class="line"><span class="attr">log.retention.hours</span>=<span class="number">168</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A size-based retention policy for logs. Segments are pruned from the log unless the remaining</span></span><br><span class="line"><span class="comment"># segments drop below log.retention.bytes. Functions independently of log.retention.hours.</span></span><br><span class="line"><span class="comment">#log.retention.bytes=1073741824</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The maximum size of a log segment file. When this size is reached a new log segment will be created.</span></span><br><span class="line"><span class="attr">log.segment.bytes</span>=<span class="number">1073741824</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The interval at which log segments are checked to see if they can be deleted according</span></span><br><span class="line"><span class="comment"># to the retention policies</span></span><br><span class="line"><span class="attr">log.retention.check.interval.ms</span>=<span class="number">300000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 服务端认证方式</span></span><br><span class="line"><span class="comment"># List of enabled mechanisms, can be more than one</span></span><br><span class="line"><span class="attr">sasl.enabled.mechanisms</span>=SCRAM-SHA-<span class="number">256</span></span><br><span class="line"><span class="comment"># Specify one of of the SASL mechanisms</span></span><br><span class="line"><span class="attr">sasl.mechanism.inter.broker.protocol</span>=SCRAM-SHA-<span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment">############################# Zookeeper #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Zookeeper connection string (see zookeeper docs for details).</span></span><br><span class="line"><span class="comment"># This is a comma separated host:port pairs, each corresponding to a zk</span></span><br><span class="line"><span class="comment"># server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot;.</span></span><br><span class="line"><span class="comment"># You can also append an optional chroot string to the urls to specify the</span></span><br><span class="line"><span class="comment"># root directory for all kafka znodes.</span></span><br><span class="line"><span class="comment"># Zookeeper 配置</span></span><br><span class="line"><span class="attr">zookeeper.connect</span>=<span class="number">192.168</span>.<span class="number">1.100</span>:<span class="number">2181</span>,<span class="number">192.168</span>.<span class="number">1.101</span>:<span class="number">2181</span>,<span class="number">192.168</span>.<span class="number">1.102</span>:<span class="number">2181</span></span><br><span class="line"><span class="comment"># 需要开启的认证，如果 ZK 开启了认证，则需要添加此项</span></span><br><span class="line"><span class="attr">zookeeper.set.acl</span>=<span class="literal">true</span></span><br><span class="line"><span class="comment"># Timeout in ms for connecting to zookeeper</span></span><br><span class="line"><span class="attr">zookeeper.connection.timeout.ms</span>=<span class="number">6000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">############################# Group Coordinator Settings #############################</span></span><br><span class="line"><span class="attr">group.initial.rebalance.delay.ms</span>=<span class="number">0</span></span><br></pre></td></tr></table></figure>

<h2 id="启动-KAFKA-服务"><a href="#启动-KAFKA-服务" class="headerlink" title="启动 KAFKA 服务"></a>启动 KAFKA 服务</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置环境变量</span></span><br><span class="line">export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/developer/kafka/kafka_2.12-2.3.1/config/kafka-server-jaas.conf&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动服务</span></span><br><span class="line">./bin/kafka-server-start.sh -daemon ./config/server.properties</span><br></pre></td></tr></table></figure>

<h2 id="客户端配置"><a href="#客户端配置" class="headerlink" title="客户端配置"></a>客户端配置</h2><figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">sasl.jaas.config</span>=org.apache.kafka.common.security.scram.ScramLoginModule required username=<span class="string">&quot;kafka-user&quot;</span> password=<span class="string">&quot;kafka-user&quot;</span><span class="comment">;</span></span><br><span class="line"><span class="attr">security.protocol</span>=SASL_PLAINTEXT</span><br><span class="line"><span class="attr">sasl.mechanism</span>=SCRAM-SHA-<span class="number">256</span></span><br></pre></td></tr></table></figure>

<h1 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h1><h2 id="创建-Topic"><a href="#创建-Topic" class="headerlink" title="创建 Topic"></a>创建 Topic</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置环境变量</span></span><br><span class="line">export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/developer/kafka/kafka_2.12-2.3.1/config/kafka-server-jaas.conf&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建 Topic</span></span><br><span class="line">./bin/kafka-topics.sh --zookeeper 192.168.1.100:2181  --replication-factor 2 --partitions 3 --topic test_topic --create</span><br></pre></td></tr></table></figure>

<h2 id="测试发送数据"><a href="#测试发送数据" class="headerlink" title="测试发送数据"></a>测试发送数据</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置环境变量</span></span><br><span class="line">export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/developer/kafka/kafka_2.12-2.3.1/config/kafka-server-jaas.conf&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">消费数据</span></span><br><span class="line">sh ./bin/kafka-producer-perf-test.sh --producer-props bootstrap.servers=192.168.1.102:9092  --producer.config ./config/producer.config --topic test_topic --num-records 10 --throughput 10 --record-size 100</span><br></pre></td></tr></table></figure>

<h2 id="测试消费数据"><a href="#测试消费数据" class="headerlink" title="测试消费数据"></a>测试消费数据</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置环境变量</span></span><br><span class="line">export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/developer/kafka/kafka_2.12-2.3.1/config/kafka-server-jaas.conf&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">消费数据</span></span><br><span class="line">sh ./bin/kafka-console-consumer.sh --consumer.config ./config/producer.config --bootstrap-server 192.168.1.101:9092 --topic test_topic --from-beginning</span><br></pre></td></tr></table></figure>

<h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul>
<li><a href="https://docs.oracle.com/javase/8/docs/technotes/guides/security/jaas/JAASRefGuide.html">https://docs.oracle.com/javase/8/docs/technotes/guides/security/jaas/JAASRefGuide.html</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/ZOOKEEPER/ZooKeeper+and+SASL">https://cwiki.apache.org/confluence/display/ZOOKEEPER/ZooKeeper+and+SASL</a></li>
<li><a href="https://zookeeper.apache.org/doc/r3.6.3/zookeeperAdmin.html">https://zookeeper.apache.org/doc/r3.6.3/zookeeperAdmin.html</a></li>
</ul>
]]></content>
      <categories>
        <category>kafka</category>
        <category>zookeeper</category>
        <category>sasl</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>zookeeper</tag>
        <tag>sasl</tag>
      </tags>
  </entry>
</search>
